{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apal9569/Banking_Behaviour/blob/master/CMPE_260_PPO_Partial_Observation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSnP6dcB9UwC"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "batch_size = 50\n",
        "epochs = 1000\n",
        "learning_rate = 1e-2\n",
        "hidden_size = 8\n",
        "n_layers = 2\n",
        "gamma = 0.99\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolModel(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(PolModel, self).__init__()\n",
        "        # self.fc1 = nn.Linear(in_dim, 64)\n",
        "        self.fc2 = nn.LSTM(in_dim, 16 , 2, bidirectional=True)\n",
        "        self.mu = nn.Linear(32, 1)\n",
        "        self.std = nn.Linear(32,1)\n",
        "\n",
        "        self.tanh  = nn.Tanh()\n",
        "        self.relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x, h, training):\n",
        "\n",
        "        # x = self.relu(self.fc1(x))\n",
        "        if h is None:\n",
        "          x, h = self.fc2(x)\n",
        "        else: \n",
        "          x, h = self.fc2(x,h)\n",
        "          # h = h[0][-1]\n",
        "        mu = self.tanh(self.mu(x))\n",
        "        std = torch.exp(self.tanh(self.std(x)))\n",
        "        dist = Normal(mu, std)\n",
        "  \n",
        "        return dist, h\n",
        "    def sample_act(self,state, h,  training):\n",
        "        dist, h = self.forward(state, h, training)\n",
        "        action = dist.sample()\n",
        "        return dist, torch.clamp(action, -2., 2.), h\n",
        "    \n",
        "class ValModel(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(ValModel, self).__init__()\n",
        "        self.fc1 = nn.LSTM(in_dim, 16 , 2, bidirectional=True)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x, h, training):\n",
        "      if h is None:\n",
        "        x, h = self.fc1(x)\n",
        "      else:\n",
        "        x, h = self.fc1(x, h)\n",
        "        # h = h[0][-1]\n",
        "      x = self.fc2(x)\n",
        "      return x, h\n",
        "    \n",
        "class History:\n",
        "    def __init__(self):\n",
        "        self.states, self.actions, self.rewards, self.log_probs, self.values, self.dones = [], [], [], [], [], []\n",
        "    \n",
        "    def append(self, state, action, log_prob, value, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.values.append(value)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states.clear()\n",
        "        self.actions.clear()\n",
        "        self.rewards.clear()\n",
        "        self.dones.clear()\n",
        "        self.log_probs.clear()\n",
        "        self.values.clear()\n"
      ],
      "metadata": {
        "id": "805HJExU9b3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a6fd0ce-8f2e-4d1f-dea4-90a1186a2179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pendulum-v1\")\n",
        "gamma = 0.9\n",
        "epsilon = 0.2\n",
        "\n",
        "data_collections = 500\n",
        "train_epoch = 30\n",
        "obs_dim = 1\n",
        "hidden_weights_actor = None\n",
        "hidden_weights_critic = None"
      ],
      "metadata": {
        "id": "4fm1LddZ9gXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e4a424-1114-4139-db1c-07117742ad44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataCollection():\n",
        "    score = 0\n",
        "    state = np.reshape(env.reset()[2], (1, -1))\n",
        "\n",
        "    for _ in range(data_collections):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        global hidden_weights_critic \n",
        "        global hidden_weights_actor\n",
        "        dist, action, hidden_weights_actor = polModel.sample_act(state, hidden_weights_actor, training=False)\n",
        "\n",
        "        value, hidden_weights_critic = valModel(state, hidden_weights_critic, training=False)\n",
        "        # print(value)\n",
        "\n",
        "        action_taken = action.detach().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action_taken)\n",
        "\n",
        "        next_state = np.reshape(next_state[2], (1, -1))\n",
        "        reward = np.reshape(reward, (1, -1))\n",
        "        done = np.reshape(done, (1, -1))\n",
        "        # print(value)\n",
        "\n",
        "        history.append(state, action, dist.log_prob(action), value, torch.FloatTensor(reward).to(device), torch.FloatTensor(1 - done).to(device))\n",
        "        \n",
        "        state = next_state\n",
        "        score += reward[0][0]\n",
        "\n",
        "        if done[0][0]:\n",
        "            scores.append(score)\n",
        "            score = 0\n",
        "            state = env.reset()[2]\n",
        "            state = np.reshape(state, (1, -1))\n",
        "    \n",
        "    value, _ = valModel(torch.FloatTensor(next_state), hidden_weights_critic, training=False)\n",
        "    history.values.append(value)\n"
      ],
      "metadata": {
        "id": "dQ2jHBBZ9jfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(config):\n",
        "\n",
        "    Qs = []\n",
        "    Adv = 0\n",
        "    ####Considered lamda = 1\n",
        "    for i in reversed(range(len(history.rewards))):\n",
        "        # print(history.rewards[i] ,gamma, history.values[i + 1], history.dones[i], history.values[i])\n",
        "        delta = (history.rewards[i] + float(gamma) * history.values[i + 1][0] * history.dones[i] - history.values[i])\n",
        "        Adv = delta + gamma * history.dones[i] * Adv\n",
        "        Qs.insert(0, Adv + history.values[i])\n",
        "        \n",
        "    actor_losses, critic_losses = [], []\n",
        "\n",
        "    states = torch.cat(history.states).view(-1, obs_dim)\n",
        "    actions = torch.cat(history.actions)\n",
        "    Qs = torch.cat(Qs).detach()\n",
        "    log_probs = torch.cat(history.log_probs).detach()\n",
        "    values = torch.cat(history.values).detach()\n",
        "    advantages = Qs - values[:-1] \n",
        "\n",
        "    for _ in range(train_epoch):\n",
        "        for batch_use in range(states.size(0) // batch_size):\n",
        "          stop = states.size(0) if batch_size * (batch_use+1) > states.size(0) else batch_size * (batch_use+1)\n",
        "          ids = np.arange(batch_size * batch_use, batch_size * (batch_use+1), 1)\n",
        "          state = states[ids, :]\n",
        "          action = actions[ids]\n",
        "          Q = Qs[ids]\n",
        "          old_log_prob = log_probs[ids]\n",
        "          advantage = advantages[ids]\n",
        "          global hidden_weights_actor \n",
        "          global hidden_weights_critic\n",
        "          dist, _, hidden_weights_actor  = polModel.sample_act(state, hidden_weights_actor, training=True)\n",
        "          # print(hidden_weights_actor)\n",
        "          hidden_weights_actor0 = torch.Tensor(copy.deepcopy(hidden_weights_actor[0].clone().detach().numpy()))\n",
        "          hidden_weights_actor1 = torch.Tensor(copy.deepcopy(hidden_weights_actor[1].clone().detach().numpy()))\n",
        "          hidden_weights_actor = tuple([hidden_weights_actor0, hidden_weights_actor1])\n",
        "          cur_log_prob = dist.log_prob(action)\n",
        "          ratio = torch.exp(cur_log_prob - old_log_prob)\n",
        "\n",
        "          entropy = dist.entropy().mean()\n",
        "\n",
        "          loss =  advantage * ratio\n",
        "\n",
        "          cur_value, hidden_weights_critic = valModel(state, hidden_weights_critic, training=True)\n",
        "          hidden_weights_critic0 = torch.Tensor(copy.deepcopy(hidden_weights_critic[0].detach().numpy()))\n",
        "          hidden_weights_critic1 = torch.Tensor(copy.deepcopy(hidden_weights_critic[1].detach().numpy()))\n",
        "          hidden_weights_critic = tuple([hidden_weights_critic0, hidden_weights_critic1])\n",
        "\n",
        "          critic_loss = torch.mean(torch.square(Q - cur_value))\n",
        "          critic_loss_copy = copy.deepcopy(critic_loss.clone().detach().numpy())\n",
        "\n",
        "          if config[0]:\n",
        "            if config[1]:\n",
        "              clipped_loss = (torch.clamp(ratio, 1. - epsilon, 1. + epsilon) * advantage - entropy * 0.005 - critic_loss_copy * 0.005)\n",
        "              actor_loss = -torch.mean(torch.min(loss, clipped_loss))\n",
        "            else:\n",
        "              clipped_loss = (torch.clamp(ratio, 1. - epsilon, 1. + epsilon)  - entropy * 0.005 - critic_loss_copy * 0.005)\n",
        "              actor_loss = -torch.mean(torch.min(loss, clipped_loss))\n",
        "          else:\n",
        "            if config[1]:\n",
        "              actor_loss = -torch.mean(loss) - ( - entropy * 0.005 )\n",
        "            else:\n",
        "              actor_loss = -torch.mean(ratio) - ( - entropy * 0.005 )\n",
        "          \n",
        "        \n",
        "          pol_optimizer.zero_grad()\n",
        "          actor_loss.backward()\n",
        "          pol_optimizer.step()\n",
        "\n",
        "          val_optimizer.zero_grad()\n",
        "          critic_loss.backward()\n",
        "          val_optimizer.step()\n",
        "\n",
        "          actor_losses.append(actor_loss.item())\n",
        "          critic_losses.append(critic_loss.item())\n",
        "    actor_loss = sum(actor_losses) / len(actor_losses)\n",
        "    critic_loss = sum(critic_losses) / len(critic_losses)\n",
        "    total_actor_loss.append(actor_loss)\n",
        "    total_critic_loss.append(critic_loss)\n",
        "    history.clear()\n"
      ],
      "metadata": {
        "id": "1UIqll6q9oUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _plot_train_history():\n",
        "    data = [scores, np.array(total_actor_loss) + np.array(total_critic_loss)]\n",
        "    labels = [\"Score\",\n",
        "              \"Total Loss\"]\n",
        "    clear_output(True)\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.plot(data[i])\n",
        "        ax.set_title(labels[i])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CtsBYixh9slO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9pXjCCt9xG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################### KEEEP THIS FOR EQ-9 ################################\n",
        "## I ran this part w.r.t to the loss function mentioned in Eq 9\n",
        "for config in [[True, True]]:\n",
        "  print(config)\n",
        "  polModel = PolModel(obs_dim).to(device)\n",
        "  valModel = ValModel(obs_dim).to(device)\n",
        "  pol_optimizer = optim.Adam(polModel.parameters(), lr=1e-3)\n",
        "  val_optimizer = optim.Adam(valModel.parameters(), lr=5e-3)\n",
        "  history = History()\n",
        "  total_actor_loss = []\n",
        "  total_critic_loss = []\n",
        "  scores = []\n",
        "\n",
        "  for step in range(epochs):\n",
        "    dataCollection()\n",
        "    trainModel(config)\n",
        "    if step%10 == 0:\n",
        "      _plot_train_history()\n",
        "    \n",
        "  # _plot_train_history()"
      ],
      "metadata": {
        "id": "rh9Px-g3NLOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _plot_train_history():\n",
        "    data = [scores, np.array(total_actor_loss) + np.array(total_critic_loss)]\n",
        "    labels = [\"Score\",\n",
        "              \"Total Loss\"]\n",
        "    clear_output(True)\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.plot(data[i])\n",
        "        ax.set_title(labels[i])\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5uX0Mc9RX9p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_plot_train_history()"
      ],
      "metadata": {
        "id": "MCptEuDvCtty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0T1R5wVX-0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}